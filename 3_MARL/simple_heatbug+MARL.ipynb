{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNbHE0phJmt0PuoMamel6U0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","**HeatBug Native Model** typically operates based on simple rules (e.g., approaching heat sources, avoiding obstacles), with its state space limited to its own position and local temperature values. In contrast, **MARL (Multi-Agent Reinforcement Learning) requires an extended complex state space**. The key differences between the two are as follows:  \n","\n","1. **Rule-Driven vs. Learning-Driven**  \n","    - The native model relies on predefined rules, eliminating the need for users to design a state space.  \n","    - MARL demands active definition of state variables (e.g., collaboration signals, adversary intentions) and handling of high-dimensional data.  \n","\n","2. **Local Perception vs. Global Coordination**  \n","    - HeatBug's decisions depend solely on its own perception, while MARL requires encoding multi-agent coordination logic (e.g., shared goals, role allocation) into the state space.  \n","\n","3. **Static Environment vs. Dynamic Environment**  \n","    - The native model operates in a fixed environment, whereas MARL must address state uncertainty arising from dynamic changes in other agents' strategies."],"metadata":{"id":"hkC2J5ALep_K"}},{"cell_type":"markdown","source":["\n","\n","# HeatBug-MARL Integration Framework  \n","\n","## **Core Design Principles**  \n","1. **Decentralized Autonomy**: Each HeatBug acts as an independent RL agent  \n","2. **Local Observations**: Partial observability (3x3 thermal grid + neighbor count)  \n","3. **Dynamic Coupling**: Collective heat generation affects individual rewards  \n","\n","---\n","\n","## **MARL Components**  \n","\n","### 1. **State Representation**  \n","- **Local Thermal Map**: 3x3 temperature matrix centered at agent  \n","- **Self Status**:  \n","  - Current energy level (normalized)  \n","  - Distance to ideal temperature: $|T_{current} - T_{ideal}|$  \n","- **Neighbor Context**:  \n","  - Number of bugs in perception range (Moore neighborhood)  \n","\n","### 2. **Action Space**  \n","- Maintain original movement options: `{N, S, E, W, Stand}`  \n","- Add communication primitive: `Broadcast_Thermal_Signal` (optional)  \n","\n","### 3. **Reward Mechanism**  \n","$$ R_t = \\underbrace{w_1 e^{-|T_t - T_{ideal}|}}_{\\text{Thermal Fit}} - \\underbrace{w_2 \\Delta E}_{\\text{Energy Cost}} + \\underbrace{w_3 \\sum_{n \\in Neighbors} \\frac{1}{d_n^2}}_{\\text{Social Term}}$$  \n","- Adaptive weights: $w_1 + w_2 + w_3 = 1$  \n","\n","### 4. **Learning Architecture**  \n","- **Independent Q-Learning**:  \n","  ```python\n","  class HeatBugRL(HeatBug):\n","      def update_q(self, state, action, reward, next_state):\n","          q_val = self.q_table[state][action]\n","          max_next = np.max(self.q_table[next_state])\n","          new_q = (1 - ALPHA) * q_val + ALPHA * (reward + GAMMA * max_next)\n","          self.q_table[state][action] = new_q\n","  ```\n","- **Centralized Critic** (Optional): Shared value function for thermal stability  \n","\n","---\n","\n","## **Emergent Behavior Targets**  \n","1. **Phase Transition**:  \n","   - From random exploration → coordinated thermal regulation  \n","2. **Self-Organized Patterns**:  \n","   - Dynamic cluster formation/ dispersion responding to global temperature  \n","3. **Energy-Efficient Migration**:  \n","   - Learned trade-off between movement cost and thermal optimization  \n","\n","\n","> **Key Insight**: Combines ABM's environmental dynamics with MARL's adaptive decision-making, creating a testbed for studying emergent coordination in physical-coupled multi-agent systems."],"metadata":{"id":"rpPjfQuYfL-N"}},{"cell_type":"markdown","source":["---\n","\n","# **Implementation Roadmap**  \n","\n"],"metadata":{"id":"drdvE3DlfYvN"}},{"cell_type":"markdown","source":["\n","## Phase 1: Baseline Integration  \n","1. Augment `HeatBug` class with Q-tables  \n","2. Modify `decide_movement()` to use ε-greedy policy  \n","3. Add reward calculation to `move()` method  \n"],"metadata":{"id":"pyZ07ya8fYtc"}},{"cell_type":"code","source":["# Phase 1: Baseline Integration\n","class HeatBugMARL(HeatBug):\n","    def __init__(self):\n","        self.q_table = defaultdict(lambda: np.zeros(5))  # 5 actions\n","        self.epsilon = 0.1\n","\n","    def decide_movement(self):\n","        if random() < self.epsilon:\n","            return random_action()\n","        else:\n","            return np.argmax(self.q_table[current_state])\n","\n","    def move(self, action):\n","        energy_cost = calculate_energy(action)\n","        new_pos = apply_movement(action)\n","        reward = self.calculate_reward(new_pos, energy_cost)\n","        return new_pos, reward"],"metadata":{"id":"z67PgOfIgFPd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Phase 2: Thermal-Aware Learning  \n","1. Implement state encoder: 3x3 temp grid → discrete state ID  \n","2. Design reward shaping functions  \n","3. Add experience replay buffer (prioritized by temp deviation)  \n","\n"],"metadata":{"id":"y_4gVRNgf-Ig"}},{"cell_type":"code","source":["# Phase 2: Thermal-Aware Learning\n","class StateEncoder:\n","    def __call__(self, temp_grid):  # 3x3 grid → int\n","        return hash(str(temp_grid.flatten()))\n","\n","class RewardShaper:\n","    def __call__(self, temp_diff, energy, social=0):\n","        return 0.6*exp(-temp_diff) - 0.3*energy + 0.1*social\n","\n","class ReplayBuffer:\n","    def add(self, state, action, reward, next_state, priority):\n","        self.buffer.append( (state, action, reward, next_state) )\n","        self.priorities.append(abs(priority))  # Temp deviation"],"metadata":{"id":"pKaWFhc3gLP0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Phase 3: Collective Adaptation  \n","1. Introduce social reward component  \n","2. Test with varying agent densities (5-50 bugs)  \n","3. Analyze emergent heat distribution patterns  \n","\n","\n",""],"metadata":{"id":"YcXh5_YTgBtI"}},{"cell_type":"code","source":["\n","# Phase 3: Collective Adaptation\n","def social_reward(neighbors):\n","    return len(neighbors) * 0.05 if 0 < len(neighbors) < 4 else -0.1\n","\n","def test_density(density):\n","    env = HeatGrid()\n","    env.add_agents(density)  # 5-50 agents\n","    return env.run_simulation()"],"metadata":{"id":"oIggOoaFgL4a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Evaluation Metrics**  \n","1. **Individual Level**:  \n","   - Energy efficiency: (ΔT achieved)/(Energy spent)  \n","   - Thermal satisfaction rate: % time in ±2°C of $T_{ideal}$  \n","2. **Collective Level**:  \n","   - Global temp variance across grid  \n","   - Cluster stability index (Lifespan of bug groups)"],"metadata":{"id":"xX_ebN3rgDUS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGgk6-KfbVKp"},"outputs":[],"source":["# Evaluation Metrics\n","def energy_efficiency(bug):\n","    return bug.total_temp_change / (bug.energy_used + 1e-6)\n","\n","def cluster_stability(clusters):\n","    lifetimes = [c.end_step - c.start_step for c in clusters]\n","    return np.mean(lifetimes)"]}]}