{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMCILosetHresOowgwcPd+y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This section demonstrates the minimax-Q learning algorithm using a simple two-player zero-sum Markov game modeled after the game of soccer.\n","# Markov Games for Multi-Agent RL: Littman's Soccer Experiment  \n","**Based on \"Markov Games as a Framework for Multi-Agent RL\" (Littman, 1994)**  \n","\n","---\n","---\n","\n","## **Core Experiment Design**  \n","\n","### 1. **Environment Definition (SOCCER)**  \n","- **Grid Structure**: 4x5 grid, with Player A starting at (2,1), Player B at (2,5), and ball possession randomly assigned.  \n","- **Objective**: Score by moving the ball into the opponent's goal (A left, B right). The environment resets to initial positions after a goal.  \n","- **Action Space**: Each player can choose actions `{N, S, E, W, Stand}`. Action execution order is random (50% chance Player A moves first).  \n","- **Ball Possession Rules**:  \n","  - Moving outside the grid results in the action failing, and the player remains in place.  \n","  - Moving to the opponent's position transfers possession to the stationary player, and the move fails.  \n","  - After scoring, ball possession is randomly assigned to either player.  \n","\n","### 2. **Reward Mechanism**  \n","- **Zero-Sum Rewards**:  \n","  - The scorer receives $+1$, and the opponent receives $-1$.  \n","  - Discount factor $\\gamma = 0.9$.  \n","\n","### 3. **Algorithm Comparison**  \n","- **Minimax-Q**:  \n","  - Explicitly models the joint action space $(a_A, a_B)$.  \n","  - Policy update: $\\max_{\\pi_A} \\min_{\\pi_B} \\mathbb{E}[Q(s,a_A,a_B)]$.  \n","- **Q-learning**:  \n","  - Assumes the opponent's policy is fixed (against random) or is an independent learner.  \n","  - Update rule: $Q(s,a_A) \\leftarrow (1-\\alpha)Q + \\alpha[R + \\gamma \\max_{a'_A}Q(s',a'_A)]$.  \n","\n","### 4. **Training Configuration**  \n","- **Learning Rate**: $\\alpha$ decays from 0.2 to 0.01 (1 million training steps).  \n","- **Exploration Strategy**: $\\epsilon$ decays from 1.0 according to $\\epsilon_t = \\epsilon_0 \\exp(-\\lambda t)$ to 0.9999954.  \n","- **Convergence Condition**: Variance of policy changes over 1000 consecutive steps is less than 0.001.  \n","- **Training Opponents**:  \n","  - MR/QR: Fixed random policy (uniform action selection).  \n","  - MM/QQ: Independent learners (separate Q-tables).  \n","\n","## **Key Function Requirements**  \n","1. **State Encoder**:  \n","   - Input: Coordinates $(x_A, y_A, x_B, y_B)$ and ball possession flag.  \n","   - Output: Unique ID (e.g., $x_A + 4y_A + 16x_B + 64y_B + 256b$).  \n","2. **Asynchronous Action Executor**:  \n","   - Input: $(a_A, a_B)$.  \n","   - Output: Results of random-order execution (including collision detection).  \n","3. **Policy Mixer**:  \n","   - Input: $Q(s,\\cdot,\\cdot)$ matrix.  \n","   - Output: Nash equilibrium strategy pair $(\\pi_A, \\pi_B)$.  \n","4. **Convergence Detector**:  \n","   - Input: Historical policies $\\{\\pi_t\\}_{t=1}^T$.  \n","   - Output: Sliding window variance (window size = 1000).  \n","5. **Adversarial Evaluator**:  \n","   - Input: Policy A vs. Policy B.  \n","   - Output: Win rate, average step length, and discounted reward difference.\n","\n","\n","#### References\n","- [Markov games as a framework for multi-agent reinforcement learning\n","Michael L. Littma](https://courses.cs.duke.edu/spring07/cps296.3/littman94markov.pdf)\n","- [Rock, Paper, Scissors Kaggle Competition](https://www.kaggle.com/c/rock-paper-scissors)"],"metadata":{"id":"Vp1rIRS-5mNW"}},{"cell_type":"markdown","source":["---\n","\n","## **MARL Modeling Phases**  \n"],"metadata":{"id":"TMKVjMdaWvP2"}},{"cell_type":"markdown","source":["### **Phase 1: Environment Dynamics Modeling**  \n","- **State Representation**:  \n","  - Coordinate encoding: $(x_A, y_A, x_B, y_B, ball\\_owner)$ hashed into a unique ID  \n","  - Goal status: Dynamically detected based on position  \n","- **Conflict Handling**:  \n","  - Asynchronous action executor randomly orders $(a_A, a_B)$  \n","  - Positional conflict triggers ball possession transfer  \n","\n","#### Code tips"],"metadata":{"id":"hmABaAeTaOS0"}},{"cell_type":"code","source":["## **Phase 1: Environment Dynamics Modeling Pseudo Code**\n","class SoccerEnv:\n","    def _encode_state(self):\n","        # Coordinate hash encoding: (x_A, y_A, x_B, y_B, ball_owner) â†’ int\n","        return hash(f\"{self.pos_A}{self.pos_B}{self.ball_owner}\")\n","\n","    def _async_act(self, a_A, a_B):\n","        # Random execution order (50% chance to swap execution order)\n","        if np.random.rand() < 0.5:\n","            first_act, second_act = a_A, a_B\n","        else:\n","            first_act, second_act = a_B, a_A\n","\n","        # Collision detection and ball possession transfer\n","        if self._check_collision(new_pos_first, pos_second):\n","            self.ball_owner = second_act.player_id  # Ball possession transfer"],"metadata":{"id":"pGcJuUqEZhyA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### **Phase 2: Value Function Design**  \n","- **Minimax-Q Update**:  \n","  $$Q(s,a_A,a_B) \\leftarrow (1-\\alpha)Q + \\alpha[R + \\gamma V^*(s')]$$  \n","  $$V^*(s') = \\max_{\\pi_A} \\min_{a_B} \\sum_{a_A} \\pi_A(a_A)Q(s',a_A,a_B)$$  \n","- **Q-learning Update**:  \n","  $$Q(s,a_A) \\leftarrow (1-\\alpha)Q + \\alpha[R + \\gamma \\max_{a'_A}Q(s',a'_A)]$$  \n"],"metadata":{"id":"fbp83tpOWzHQ"}},{"cell_type":"code","source":["## **Phase 2: Value Function Design Pseudo Code**\n","def minimax_q_update(Q, s, a_A, a_B, R, s_prime):\n","    V = solve_nash(Q[s_prime])  # Solve V*(s') using linear programming\n","    Q[s][a_A][a_B] = (1-alpha)*Q[s][a_A][a_B] + alpha*(R + gamma*V)\n","\n","def q_learning_update(Q, s, a_A, R, s_prime):\n","    max_q_prime = np.max(Q[s_prime])  # Ignore opponent's actions\n","    Q[s][a_A] = (1-alpha)*Q[s][a_A] + alpha*(R + gamma*max_q_prime)"],"metadata":{"id":"yApjFQAUZzWG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Phase 3: Policy Optimization**  \n","- **Minimax Strategy**: Solve for mixed strategy $\\pi_A^* = \\arg\\max_{\\pi} \\min_{a_B} \\mathbb{E}[Q]$ using linear programming  \n","- **Exploration Mechanism**: Maintain $\\epsilon > 0.9$ during early training with $\\epsilon$-greedy  \n","\n"],"metadata":{"id":"7UNzz_VFW1xC"}},{"cell_type":"code","source":["## **Phase 3: Policy Optimization Pseudo Code**\n","def get_minimax_policy(Q, state):\n","    # Generate mixed strategy probability distribution (LP solution)\n","    payoff_matrix = Q[state]\n","    prob_A = solve_linear_program(payoff_matrix)  # Maximize minimum payoff\n","    return prob_A  # Example output: [0.3, 0.1, 0.2, 0.2, 0.2]\n","\n","def epsilon_greedy(action_probs, epsilon):\n","    if np.random.rand() < epsilon:\n","        return np.random.choice(ACTION_SPACE)  # Explore\n","    else:\n","        return np.argmax(action_probs)         # Exploit"],"metadata":{"id":"lmmg3uVEZ13U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Phase 4: Training and Evaluation**  \n","1. **Training Protocol**:  \n","   - MM/QQ groups use parallel asynchronous updates (avoid locking)  \n","2. **Evaluation Methods**:  \n","   - Random opponent test: 10,000 steps to calculate win rate (including 0.1 probability of forced draw)  \n","   - Human strategy confrontation: Deterministic rules (intercept first + straight attack)  \n","   - Challenger test: Q-learning training against a frozen champion strategy  \n","\n","\n","\n"],"metadata":{"id":"nN7sonQ_W4sD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yitQxAzGykCf"},"outputs":[],"source":["## **Phase 4: Training and Evaluation Pseudo Code**\n","def parallel_training(agent1, agent2):\n","    # Asynchronous parameter updates (lock to avoid conflicts)\n","    with threading.Lock():\n","        agent1.update_q()\n","        agent2.update_q()\n","\n","def evaluate_vs_random(policy, n_episodes=1e5):\n","    win_count = 0\n","    for _ in range(n_episodes):\n","        if env.goal_scored == 'A':\n","            win_count += 1\n","        elif np.random.rand() < 0.1:  # Inject forced draw\n","            pass\n","    return win_count / n_episodes"]}]}