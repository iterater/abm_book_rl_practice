@article{Williams1992,
  title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  volume = {8},
  ISSN = {1573-0565},
  url = {https://link.springer.com/content/pdf/10.1007/BF00992696.pdf},
  DOI = {10.1007/bf00992696},
  number = {3–4},
  journal = {Machine Learning},
  publisher = {Springer Science and Business Media LLC},
  author = {Williams,  Ronald J.},
  year = {1992},
  month = may,
  pages = {229–256}
}

@misc{CartPole,
	author = {},
	title = {{C}art {P}ole -- {G}ymnasium {D}ocumentation},
	howpublished = {\url{https://gymnasium.farama.org/environments/classic_control/cart_pole/}},
	year = {},
	note = {[Accessed 25-04-2025]},
}



@inproceedings{Ng2000ICML,
  author       = {Andrew Y. Ng and
                  Stuart Russell},
  title        = {Algorithms for Inverse Reinforcement Learning},
  booktitle    = {Proceedings of the Seventeenth International Conference on Machine
                  Learning {(ICML} 2000), Stanford University, Stanford, CA, USA, June
                  29 - July 2, 2000},
  pages        = {663--670},
  year         = {2000},
  url = {https://ai.stanford.edu/~ang/papers/icml00-irl.pdf}
}

@inproceedings{Ziebart2008AAAI,
author = {Ziebart, Brian D. and Maas, Andrew and Bagnell, J. Andrew and Dey, Anind K.},
title = {Maximum entropy inverse reinforcement learning},
year = {2008},
isbn = {9781577353683},
publisher = {AAAI Press},
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.},
booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3},
pages = {1433–1438},
numpages = {6},
location = {Chicago, Illinois},
series = {AAAI'08},
url = {https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf}
}

@incollection{Littman1994,
title = {Markov games as a framework for multi-agent reinforcement learning},
editor = {William W. Cohen and Haym Hirsh},
booktitle = {Machine Learning Proceedings 1994},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {157-163},
year = {1994},
isbn = {978-1-55860-335-6},
doi = {10.1016/B978-1-55860-335-6.50027-1},
url = {https://courses.cs.duke.edu/spring07/cps296.3/littman94markov.pdf},
author = {Michael L. Littman},
abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsis-tic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.}
}

@misc{RockPaperScissors,
	author = {},
	title = {{R}ock, {P}aper, {S}cissors -- {K}aggle},
	howpublished = {\url{https://www.kaggle.com/c/rock-paper-scissors}},
	year = {},
	note = {[Accessed 25-04-2025]},
}

@book{howard1960dynamic,
  title={Dynamic Programming and Markov Processes},
  author={Howard, Ronald A},
  year={1960},
  publisher={John Wiley}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}